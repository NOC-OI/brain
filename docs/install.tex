\documentclass{article}
\usepackage{hyperref}

\begin{document}

\frontmatter
\tableofcontents
\clearpage

{\huge {\bf Foreword}}

\section{Prerequisites}

\subsection{Setting up a Jetson}

https://www.jetson-ai-lab.com/initial_setup_jon.html

An up-to-date Jetson Nano should be able to boot the JetPack 6.2 SD card image (\href{https://developer.nvidia.com/downloads/embedded/L4T/r36_Release_v4.4/jp62-r1-orin-nano-sd-card-image.zip}{https://developer.nvidia.com/downloads/embedded/L4T/r36_Release_v4.4/jp62-r1-orin-nano-sd-card-image.zip}) straight away. Download the zip file and flash the image inside to an SD card (>128GB reccomended for BRAIN, 64GB realistic minimum) and insert into the Jetson. Once powered on it should boot directly into the OOBE.

You might need to first boot the JetPack 5.1.3 SD card image (\href{https://developer.nvidia.com/downloads/embedded/l4t/r35_release_v5.0/jp513-orin-nano-sd-card-image.zip}{https://developer.nvidia.com/downloads/embedded/l4t/r35_release_v5.0/jp513-orin-nano-sd-card-image.zip})

Once booted, make sure your system is up to date:

\begin{lstlisting}
alewin@brain2:~/git/brain$ sudo apt update
Get:1 file:/var/cudnn-local-tegra-repo-ubuntu2204-9.3.0  InRelease [1,572 B]
Get:2 file:/var/l4t-cuda-tegra-repo-ubuntu2204-12-6-local  InRelease [1,572 B]
[...]
Reading state information... Done
All packages are up-to-date.

alewin@brain2:~/git/brain$ sudo apt upgrade
Reading package lists... Done
Building dependency tree... Done
[...]
Learn more about Ubuntu Pro at https://ubuntu.com/pro
0 to upgrade, 0 to newly install, 0 to remove and 0 not to upgrade.
\end{lstlisting}

\subsection{Setting up the runtime environment}

\href{https://org.ngc.nvidia.com/setup/api-keys}{https://org.ngc.nvidia.com/setup/api-keys}

\begin{lstlisting}
user@brain:~/git/brain$ docker login nvcr.io
Username: $oauthtoken
Password: <Your Key>
\end{lstlisting}

\subsection{Testing using the NOC L4T base image}

The NOC L4T image is a pre-built docker image that contains PyTorch, torchvision and ultralytics (for using YOLO models). It also includes a self-test script as a placeholder to be overwritten with your intended application. This is useful for testing your configuration is correct. Your output should look similar to below:

\begin{lstlisting}
user@brain:~/git/brain/vision$ docker run --runtime nvidia -it brain/l4t-base:j62-r36.4-0
NOC L4T (Linux 4 Tegra) base image test script

✓ Standard dependencies loaded
✓ PyTorch CUDA support
    PyTorch Version: 2.8.0
    CUDA Version: 12.6
    CUDA Device: Orin
✓ Torchvision loaded
✓ OpenCV loaded
✓ Ultralytics YOLO loaded

If everything comes back without error, you have configured your Jetson correctly!
This is a base container, you should extend from it with your own application.
\end{lstlisting}


\end{document}
